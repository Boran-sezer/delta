import streamlit as st
from groq import Groq
import firebase_admin
from firebase_admin import credentials, firestore
import base64
import json
import re

# --- 1. INITIALISATION ---
if not firebase_admin._apps:
    try:
        encoded = st.secrets["firebase_key"]["encoded_key"].strip()
        decoded_json = base64.b64decode(encoded).decode("utf-8")
        cred = credentials.Certificate(json.loads(decoded_json))
        firebase_admin.initialize_app(cred)
    except: pass

db = firestore.client()
doc_ref = db.collection("memoire").document("profil_monsieur")
client = Groq(api_key="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")

# --- 2. R√âCUP√âRATION M√âMOIRE ---
res = doc_ref.get()
archives = res.to_dict().get("archives", {}) if res.exists else {}

# --- 3. INTERFACE ---
st.set_page_config(page_title="DELTA AI - R1", layout="wide")
st.markdown("<h1 style='color:#00d4ff;'>‚ö° SYSTEME DELTA : DEEP REASONING</h1>", unsafe_allow_html=True)

if "messages" not in st.session_state: 
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]): st.markdown(m["content"])

# --- 4. LOGIQUE DE TRAITEMENT ---
if prompt := st.chat_input("Ordres, Monsieur Sezer..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)

    # --- ANALYSEUR PAR RAISONNEMENT (DEEPSEEK R1) ---
    # Ce mod√®le va litt√©ralement "r√©fl√©chir" √† l'importance de l'info
    sys_analyse = (
        f"Tu es l'unit√© de raisonnement logique de Monsieur Sezer. M√©moire actuelle : {archives}. "
        f"Derni√®re interaction : '{prompt}'. "
        "MISSION : Analyse si ce message contient une information structurelle, technique ou personnelle vitale. "
        "Si oui, r√©organise l'enti√®ret√© du JSON pour qu'il soit optimal. Supprime l'inutile, fusionne les doublons. "
        "R√©ponds EXCLUSIVEMENT avec le JSON complet. Si rien ne justifie une modification, r√©ponds : IGNORE."
    )
    
    try:
        # Utilisation de DeepSeek-R1 pour une analyse ultra-logique
        check = client.chat.completions.create(
            model="deepseek-r1-distill-llama-70b", 
            messages=[{"role": "system", "content": "Tu es un moteur d'analyse logique de haut niveau."}, {"role": "user", "content": sys_analyse}],
            temperature=0.1 # Basse temp√©rature pour une pr√©cision maximale
        )
        verdict = check.choices[0].message.content.strip()
        
        # On extrait le JSON (DeepSeek peut inclure sa 'pens√©e' entre des balises <think>)
        json_match = re.search(r'\{.*\}', verdict, re.DOTALL)
        if json_match:
            nouvelles_archives = json.loads(json_match.group(0))
            if nouvelles_archives != archives:
                archives = nouvelles_archives
                doc_ref.set({"archives": archives})
                st.toast("üß† Raisonnement appliqu√© : M√©moire restructur√©e")
    except: pass

    # --- 5. R√âPONSE DE DELTA ---
    with st.chat_message("assistant"):
        instruction_delta = (
            f"Tu es DELTA. Tu parles √† Monsieur Sezer Boran. "
            f"Archives : {archives}. "
            "Sois percutant, froid, technique et extr√™mement efficace."
        )
        placeholder = st.empty()
        full_response = ""
        try:
            # On reste sur Llama 3.3 pour la rapidit√© de conversation
            stream = client.chat.completions.create(
                model="llama-3.3-70b-versatile", 
                messages=[{"role": "system", "content": instruction_delta}] + st.session_state.messages,
                temperature=0.3, stream=True
            )
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content
                    placeholder.markdown(full_response + "‚ñå")
            placeholder.markdown(full_response)
        except: placeholder.markdown("Liaison interrompue.")
        st.session_state.messages.append({"role": "assistant", "content": full_response})
